name: Daily Sermon Scraping

on:
  schedule:
    # Runs every day at midnight UTC (convert to your timezone)
    - cron: '0 0 * * *'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest
      
      - name: Install ChromeDriver
        uses: nanasess/setup-chromedriver@v2
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run scraping and upload
        env:
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          python ingestion/scrape_and_embed_fixed.py
      
      - name: Commit scraped data (optional backup)
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/*.json || true
          git commit -m "Auto-scraped sermons on $(date)" || true
          git push || true
        continue-on-error: true
      
      - name: Send notification on failure
        if: failure()
        run: |
          echo "Scraping failed! Check logs at https://github.com/${{ github.repository }}/actions"